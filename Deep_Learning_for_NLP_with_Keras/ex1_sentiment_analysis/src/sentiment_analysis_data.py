from data import Data
from data_multiclass import DataMultiClass
from collections import Counter
import numpy as np
import re

class SentimentAnalysisData(DataMultiClass):

    UNDEF_WORD = 'NaN'

    """
    Class for processing data for multiclass classification sentiment analysis problems
    """
    def __init__(self, x, y, test_ratio=0.1, val_ratio=0.0, k_fold=0, shuffle=True, threshold=0):
        Data.__init__(self, x, y, test_ratio, val_ratio, k_fold, shuffle)
        self.threshold = threshold
        self.vocabulary = {self.UNDEF_WORD: 0}
        self.preprocess()

    def preprocess(self):
        """
        Replace x_train, x_test, y_train, y_test by vectorized vectors, i.e.
        feature vectors for x
        one-hot vectors for y
        """
        self.init_vocabulary()
        self.set_train_x(self.index_samples(self.get_train_x()))
        self.set_test_x(self.index_samples(self.get_test_x()))
        self.vectorize_x()
        self.vectorize_y()

    def init_vocabulary(self):
        """
        Initialize vocabulary of words based on the training set, discard words with low frequency
        """
        counter = Counter()

        # count the words that occur in the training data
        for instance in self.get_train_x():
            counter.update(self.process_sample(instance))

        # initialize vocabulary discarding the words with frequency lower than threshold
        i = 1 # start from 1 since the first word in the vocabulary reserved for rare/unknown words
        for key in counter:
            if((self.threshold - counter[key]) < 0): # TODO replace by numpy array and drop by condition
                self.vocabulary[key] = i
                i += 1

    def index_samples(self, samples):
        """
        Indexes samples according to their ordinal number in the previously generated vocabulary
        :param samples: list of strings to be transformed to list of lists of indices
        :return: numpy array by shape (len(samples), 1) -
        list of lists of indices, representing sample strings
        """
        sequences = []
        for sample in samples:
            sequences.append([self.vocabulary[p]
                              if p in self.vocabulary else self.vocabulary[self.UNDEF_WORD] # UNDEF_WORD : 0
                              for p in self.process_sample(sample)])

        return sequences

    def vectorize_sequences(self, sequences):
        # Create an all-zero matrix of shape (len(sequences), dimension)
        results = np.zeros((len(sequences), len(self.vocabulary)))
        for i, sequence in enumerate(sequences):
            # sequence = np.array(sequence.astype('int'))
            results[i, sequence] = 1.  # set specific indices of results[i] to 1s
        return results

    def vectorize_x(self):
        self.set_train_x(self.vectorize_sequences(self.get_train_x()))
        self.set_test_x(self.vectorize_sequences(self.get_test_x()))

    def process_sample(self, sample):
        """
        Process a single sample - string of text by processing each word in it separately
        :param sample: a string to be parsed
        :return: a list of processed words
        """
        sample = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|' \
                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sample) # remove hyperlinks
        sample = re.sub('(@[A-Za-z0-9_]+)', '', sample) # remove replies
        return [self.process_word(word) for word in re.findall(r'\w+', sample)] # TODO something less trivial?

    def process_word(self, word):
        """
        Process a single word
        :param word: separated by spaces sequence of low-case symbols
        :return: processed word
        """
        return word

    def get_num_features(self):
        """
        :return: num of generated by the class features
        """
        return len(self.vocabulary)
