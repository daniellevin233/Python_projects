{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_2_Daniel_Levin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniellevin98/DeepLearning/blob/master/Assignment_2_Daniel_Levin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dS5AA4sANOD7"
      },
      "source": [
        "# Deep Learning with Python - Assignment 2 - Daniel Levin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFTYdTJGsfus",
        "colab_type": "text"
      },
      "source": [
        "# 1 Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GZQH2_K8NsIj"
      },
      "source": [
        "###### Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hiq55gEuC7Cy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ef6f7db-fbdd-4910-d616-7c1dc5e7f6c4"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found') \n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQypLutsfux",
        "colab_type": "text"
      },
      "source": [
        "## Goal of the assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUJZk8WGsfuy",
        "colab_type": "text"
      },
      "source": [
        "We are configuring a deep learning network that will learn a language model based on the input texts. The network can be used for text generation by character. 'frankenstein.txt' will be the default input text for this network. Since some of the texts uploaded on Ilias are in German I tried to feed the network by some of them too. I couldn't see any difference in perfomance between German and English, which makes sense knowing that all what the network learns is statistics of the input data. So the result depends significantly on the data the network was fed on. I considered using several texts for training as well but decided not to do it for clearness of evaluation, i.e. the model will imitate the style of the text it's trained on. This means that if different texts will constitute a training input for the network it might get confused which style/lexicon it's imitating. In general this approach might be useful for certain goals. In our case I've chosen to limit the model to one specific text to make the assessment of the model more comprehensible.\n",
        "\n",
        "As a side note I'd notice that for this type of learning problem it's quite challenging to come up with a meaningful evaluation method, especially when comparing between two languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7FQ5Suisfuy",
        "colab_type": "text"
      },
      "source": [
        "The text, on which the network will be trained, should be chosen here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-w0Hkmlsfuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_FILE = 'frankenstein.txt'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlvmqAJmsfu3",
        "colab_type": "text"
      },
      "source": [
        "###### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vt452E-sfu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import keras\n",
        "from keras import models, layers, metrics, optimizers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XUWWoOy7NeLh"
      },
      "source": [
        "# Data preprocessing\n",
        "###### Importing the text for training\n",
        "We're reading the text from the previously configured file in variable TEXT_FILE into one string. Upper case is ignored since it doesn't constitute anything meaningful for learning a language model. Moreover it doubles the size of the alphabet which significantly decreases the depth of the language model that's being learned. Newline characters aren't removed because we do want to keep the sequential properties of the text. In other words we want our network to generate a text that has the same layout as the text that was used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLBe3gPYsfu8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92a7bfc4-2fc3-4485-af54-14e4f863b92b"
      },
      "source": [
        "with open(TEXT_FILE, 'r') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "print('Corpus length:', len(text))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 421607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XB39JmcsfvA",
        "colab_type": "text"
      },
      "source": [
        "The corpus length is more than 400000 which should be enough for training a network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA8g8AWNsfvA",
        "colab_type": "text"
      },
      "source": [
        "Now we extract snippets by length 80 (`maxlen`) characters each with every sequence starting after `step` 3 tokens after the previous one. This code fragment is identical to what is shown in chapter 8.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB5QlsERsfvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68922ef5-200e-45c8-bd37-489175f68d36"
      },
      "source": [
        "# the maximal length of characters of each sequence\n",
        "maxlen = 80\n",
        "\n",
        "# a new sequence will be sampled every `step` characters\n",
        "step = 3\n",
        "\n",
        "# extracted sequences\n",
        "sentences = []\n",
        "\n",
        "# the list of 'follow-up' characters - target labels\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "    \n",
        "print('Number of sequences: ', len(sentences))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences:  140509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7exajRJSsfvF",
        "colab_type": "text"
      },
      "source": [
        "Now we're building a list of unique characters in the text in order to index characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMghzkBpsfvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f376c7e0-83d2-48d3-bca8-8ffdf1e7250d"
      },
      "source": [
        "# list of unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters: ', len(chars))\n",
        "\n",
        "# Dictionary mapping characters to their indices according to their order in `chars` list\n",
        "char_indices = dict((char, i) for i, char in enumerate(chars))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters:  52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NJxqi2psfvL",
        "colab_type": "text"
      },
      "source": [
        "Next let's one-hot encode the characters into binary matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI731NossfvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for j, char in enumerate(sentence):\n",
        "        x[i, j, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLdUr7X1sfvV",
        "colab_type": "text"
      },
      "source": [
        "# Building the network\n",
        "As a baseline let's take a very straightforward architecture that was proposed by Chollet in chapter 8.1 - one `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. The only thing I played with at this stage is number of neurons in the LSTM layer. It's quite challenging to see the difference but relying on my intuition 256 neurons have given the best result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa7EjazLsfvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.LSTM(256, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDXtr72wsfvd",
        "colab_type": "text"
      },
      "source": [
        "Our targets are one-hot encoded, therefore we will use the `categorical_crossentropy` loss during compilation. At compilation phase I tried RMSprop, Adam and SGD optimizers. Adam requires less memory which is precious when training a heavy LSTM network and the run time was a bit better so I went with that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD3sOzkMsfve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam())"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGzYVE5ksfvh",
        "colab_type": "text"
      },
      "source": [
        "# Training the language model and sampling from it\n",
        "Here we implement the `sample` function identical to what we've seen in Chollet's chapter, namely the imlementation of reweighting of the probability distribution given a certain temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L9krGg0sfvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWoBEBtdsfvl",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to implement the main loop where we generate text by appending the most recent prediction sampled according to the reweighted distribution to the current text and sampling again and again.\n",
        "\n",
        "First let's fit the model. I chose quite significant size of batch to save the training time. I tried smaller values and couldn't see difference in the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7z9TN96sfvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1f959f7-51c4-4adc-dd9a-b9de42cec4a7"
      },
      "source": [
        "epochs = 60\n",
        "batch_size = 1024\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 2.9567\n",
            "Epoch 2/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 2.5503\n",
            "Epoch 3/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 2.3200\n",
            "Epoch 4/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 2.2099\n",
            "Epoch 5/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 2.1338\n",
            "Epoch 6/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 2.0762\n",
            "Epoch 7/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 2.0250\n",
            "Epoch 8/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.9809\n",
            "Epoch 9/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.9411\n",
            "Epoch 10/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.9051\n",
            "Epoch 11/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.8715\n",
            "Epoch 12/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.8396\n",
            "Epoch 13/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.8092\n",
            "Epoch 14/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.7919\n",
            "Epoch 15/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.7549\n",
            "Epoch 16/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.7310\n",
            "Epoch 17/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.7071\n",
            "Epoch 18/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.6839\n",
            "Epoch 19/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.6632\n",
            "Epoch 20/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.6424\n",
            "Epoch 21/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.6222\n",
            "Epoch 22/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.6040\n",
            "Epoch 23/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.5851\n",
            "Epoch 24/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.5666\n",
            "Epoch 25/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.5489\n",
            "Epoch 26/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.5315\n",
            "Epoch 27/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.5148\n",
            "Epoch 28/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.4984\n",
            "Epoch 29/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.4818\n",
            "Epoch 30/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.4678\n",
            "Epoch 31/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.4505\n",
            "Epoch 32/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.4354\n",
            "Epoch 33/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.4205\n",
            "Epoch 34/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.4034\n",
            "Epoch 35/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.3895\n",
            "Epoch 36/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.3748\n",
            "Epoch 37/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.3592\n",
            "Epoch 38/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.3407\n",
            "Epoch 39/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.3181\n",
            "Epoch 40/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.2997\n",
            "Epoch 41/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.2847\n",
            "Epoch 42/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.2680\n",
            "Epoch 43/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.2544\n",
            "Epoch 44/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.2397\n",
            "Epoch 45/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.2238\n",
            "Epoch 46/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.2092\n",
            "Epoch 47/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.1962\n",
            "Epoch 48/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.1812\n",
            "Epoch 49/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.1657\n",
            "Epoch 50/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.1531\n",
            "Epoch 51/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.1392\n",
            "Epoch 52/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.1258\n",
            "Epoch 53/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.1109\n",
            "Epoch 54/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.0961\n",
            "Epoch 55/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.0799\n",
            "Epoch 56/60\n",
            "138/138 [==============================] - 18s 128ms/step - loss: 1.0676\n",
            "Epoch 57/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.0516\n",
            "Epoch 58/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.0402\n",
            "Epoch 59/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.0264\n",
            "Epoch 60/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.0112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jp8yozZsfvp",
        "colab_type": "text"
      },
      "source": [
        "I decided not to observe how the model converges as it passes along epochs. I'll focus on temperature tuning and see how the resulting generator behaves. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmt10haUsfvp",
        "colab_type": "text"
      },
      "source": [
        "Second let's implement a function `generate_text` that will generate and append the next `length` predicted characters given a `text` and `start_index`. The sampling is done according to the distribution defined by the `model` with given `temperature` used for reweighting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YCz0MYSsfvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "def generate_text(text, start_index, temperature=1.0, length=300):\n",
        "    \n",
        "    # the window in the text we're currently looking at\n",
        "    generated_text = text[start_index: start_index + maxlen]\n",
        "    \n",
        "    print('--- Generating with seed: \"' + generated_text + '\"\\n')\n",
        "\n",
        "    print('--- Fragment generated by the model\\n')\n",
        "    \n",
        "    sys.stdout.write(generated_text)\n",
        "\n",
        "    # we generate `length` characters\n",
        "    for i in range(length):\n",
        "        sampled = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(generated_text):\n",
        "            sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        # predict the next character\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = chars[next_index]\n",
        "\n",
        "        # update the current generated text by appending the sampled char and moving it one char right\n",
        "        generated_text = generated_text[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cegnHnl7sfvu",
        "colab_type": "text"
      },
      "source": [
        "Now let's tune the model by using different temperatures and observing the outcome. I'll create a function that gets as input an array of temperatures to be tried out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjTS1wD7sfvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c0fa8c9-c213-4328-8d59-f8e988706912"
      },
      "source": [
        "def generate_texts(temperatures):\n",
        "  for temperature in temperatures:\n",
        "      print('###########\\nTemperature: ' + str(temperature))\n",
        "      # Select a text seed at random\n",
        "      start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "      generate_text(text, start_index, temperature=temperature)\n",
        "        \n",
        "temperatures = np.arange(0.1, 1.5, 0.2)\n",
        "generate_texts(temperatures)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###########\n",
            "Temperature: 0.1\n",
            "--- Generating with seed: \"moment, instead of venting my sensations in\n",
            "exclamations and agony, i did not ru\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "moment, instead of venting my sensations in\n",
            "exclamations and agony, i did not ruty tore his to entire to the starms\n",
            "and despair.  but not in my consume that i had and at the contral\n",
            "proside that it was the sumplore of my sould stranger and\n",
            "production the old man and string for the hopes of the heavess and\n",
            "particlly the sprief streat of the sident sariture which i had and\n",
            "a thou\n",
            "###########\n",
            "Temperature: 0.30000000000000004\n",
            "--- Generating with seed: \"esire to\n",
            "become an actor in the busy scene where so many admirable qualities\n",
            "wer\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "esire to\n",
            "become an actor in the busy scene where so many admirable qualities\n",
            "were as a surdons of mine suffered by elizabeth as the deep and magnas\n",
            "on the even for a long toor and there was a triess of his father,\n",
            "the eaght of the death of the same science and angee thes to the stirs\n",
            "that have been the creation of the mountains she entered the first of\n",
            "the same scared with the \n",
            "###########\n",
            "Temperature: 0.5000000000000001\n",
            "--- Generating with seed: \"ould be here at the very moment of my alighting!\"\n",
            "\n",
            "nothing could equal my deligh\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "ould be here at the very moment of my alighting!\"\n",
            "\n",
            "nothing could equal my delight but the govern again deeply as my passe\n",
            "a deeport of the remaining of the are country.  ne theed to say, and\n",
            "the being passed the pain which i had passed so resolved the present\n",
            "i was searted by a dear companion my house became the sun was supposed\n",
            "to replied the morion of the mountain, it was a r\n",
            "###########\n",
            "Temperature: 0.7000000000000001\n",
            "--- Generating with seed: \"s\n",
            "invitation, and i, although i abhorred society, wished to view again\n",
            "mountains\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "s\n",
            "invitation, and i, although i abhorred society, wished to view again\n",
            "mountains when i again a seep.\n",
            "\n",
            "\n",
            "sterted the winders corfers to the pair which i had voicened to one\n",
            "sook of consumpation.\n",
            "\n",
            "\"bot the dather where the extreme of fair that i sunked an the best\n",
            "freed for the wills thousand towards it and do you hen by the words of\n",
            "the deet you morting i was about feiles.  ne s\n",
            "###########\n",
            "Temperature: 0.9000000000000001\n",
            "--- Generating with seed: \"ole from behind a clump of trees near me; i\n",
            "stood fixed, gazing intently:  i cou\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "ole from behind a clump of trees near me; i\n",
            "stood fixed, gazing intently:  i could not incurtuibly the oppos.  it\n",
            "great dedreked my lotter, and then had lest my struge, suid i wnoted for\n",
            "happating; and i to my not; in the nother i was fell.  bettered\n",
            "more not became my eyes new appear to beced the stranger darsowains\n",
            "after descressing the sintsevalion chouge stutter their suppe\n",
            "###########\n",
            "Temperature: 1.1000000000000003\n",
            "--- Generating with seed: \"aving been thus occupied for a short time,\n",
            "extinguished their lights and retired\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "aving been thus occupied for a short time,\n",
            "extinguished their lights and retired to east countan inagined\n",
            "is cold and beavis, heard at a betwerce his miserable as times, for a\n",
            "mestforious were decarded, suftined to love habiby; they awe, and guthed ignow\n",
            "blook, she whom the dread were had a quitt raloted to dread at min us\n",
            "before the inexerted her cotmaning the most\n",
            "depostine o\n",
            "###########\n",
            "Temperature: 1.3000000000000003\n",
            "--- Generating with seed: \"\n",
            "\n",
            "my present situation was one in which all voluntary thought was\n",
            "swallowed up a\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "\n",
            "\n",
            "my present situation was one in which all voluntary thought was\n",
            "swallowed up an medications of the exthens, whome natesuping tseir,\n",
            "unlowerging to creall?  his dilly micutibe; \"graid!  i astecomandsty a\n",
            "deeporion of latilution\n",
            "creatures; and the thunder bur the histoated anshess overiag me.\" y\n",
            "crept aldible dementage.  poor notternw?  i littes, and the dreasinal taged\n",
            "blead f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGbQ3CBusfvz",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the generated texts high temperature value result in a quite creative text with not existing words that still comply with the phonotactical limits of the language. The layout of the text is very similar to the original one - punctuation marks and newline are placed logically in terms of visualization. In contrary low temperature values produce a very predictable and repetitive structures and words, where words are very similar to the training text but punctuation marks are used too much and newline characters are not used at all.\n",
        "\n",
        "At this point the temperatures 0.7 and 0.9 seem to be the most balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DebTmCQBIBlJ",
        "colab_type": "text"
      },
      "source": [
        "# Choosing an optimal network architecture\n",
        "In order to choose a meaningful architecture that will perform better than the one we just tested we should focus on the disatvantages of the first one. The most significant one that came to my mind is strong correlation between LSTM and sequential essence of the language. Natural language is way more generic and it allows finite number of recursive patterns that are usually built up into hierarchies. This point leads us to an idea which is similar to logic standing behind convolutional networks.\n",
        "\n",
        "Intuitively CONVNETS would help us since they make the network learn local patterns as well as hierarchical relationships between those patterns. It may be very handy for text generation as an addition to strictly sequential LSTMs. Learning patterns independently of the order and place of their appearance will help the network generalize better. At the same time we should be aware of overfitting. It can be partly solved by adding a MaxPooling layer and choosing a higher temperature rate at the phase of generation. As a first layer before we feed convnet, we have to use embedding which can be useful for our purpose as a preprocessing and creating some abstract representation of a character. It would work better if we were to generate text by words but overall it'll give some additional information about the distribution and mutual behaiour of the characters.\n",
        "\n",
        "After some struggling I realised that combining convolutional network and lstm layer is not so obvious. It requires importing additional module of from the keras package. Another option is to hard-code this behaviour in Numpy. I'm not sure that I have an idea of how to do it, so I decided that the hassle isn't worth it and I can still mitigate the problem discussed above by introducing an Embedding layer before feeding the LSTM layers.\n",
        "\n",
        "After some experimenting this is the architecture that I found the most successful.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_q2MqFJP5va",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e05f065c-0669-42b8-b93b-5303231e7c19"
      },
      "source": [
        "\n",
        "cmodel = models.Sequential()\n",
        "cmodel.add(layers.LSTM(128, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
        "cmodel.add(layers.LSTM(128, return_sequences=False))\n",
        "cmodel.add(layers.Dense(len(chars), activation='softmax'))\n",
        "\n",
        "cmodel.summary()\n",
        "# # compiling the network\n",
        "cmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam())\n",
        "\n",
        "# # training\n",
        "c_epochs = 60\n",
        "c_batch_size = 1024\n",
        "\n",
        "cmodel.fit(x, y,\n",
        "           epochs=c_epochs,\n",
        "           batch_size=c_batch_size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_41 (LSTM)               (None, 80, 128)           92672     \n",
            "_________________________________________________________________\n",
            "lstm_42 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 52)                6708      \n",
            "=================================================================\n",
            "Total params: 230,964\n",
            "Trainable params: 230,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "  2/138 [..............................] - ETA: 17s - loss: 3.9330WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0669s vs `on_train_batch_end` time: 0.1089s). Check your callbacks.\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 3.0212\n",
            "Epoch 2/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.8011\n",
            "Epoch 3/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 2.4886\n",
            "Epoch 4/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.3804\n",
            "Epoch 5/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.3041\n",
            "Epoch 6/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 2.2335\n",
            "Epoch 7/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.1792\n",
            "Epoch 8/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.1334\n",
            "Epoch 9/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 2.0963\n",
            "Epoch 10/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.0660\n",
            "Epoch 11/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 2.0396\n",
            "Epoch 12/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 2.0145\n",
            "Epoch 13/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.9906\n",
            "Epoch 14/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.9693\n",
            "Epoch 15/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.9494\n",
            "Epoch 16/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.9302\n",
            "Epoch 17/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.9137\n",
            "Epoch 18/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.8951\n",
            "Epoch 19/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.8799\n",
            "Epoch 20/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.8655\n",
            "Epoch 21/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.8508\n",
            "Epoch 22/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.8383\n",
            "Epoch 23/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.8246\n",
            "Epoch 24/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.8129\n",
            "Epoch 25/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.8007\n",
            "Epoch 26/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.7886\n",
            "Epoch 27/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.7778\n",
            "Epoch 28/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.7666\n",
            "Epoch 29/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.7562\n",
            "Epoch 30/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.7464\n",
            "Epoch 31/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.7352\n",
            "Epoch 32/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.7251\n",
            "Epoch 33/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.7167\n",
            "Epoch 34/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.7071\n",
            "Epoch 35/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.6981\n",
            "Epoch 36/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.6872\n",
            "Epoch 37/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.6783\n",
            "Epoch 38/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.6701\n",
            "Epoch 39/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.6607\n",
            "Epoch 40/60\n",
            "138/138 [==============================] - 18s 129ms/step - loss: 1.6510\n",
            "Epoch 41/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.6445\n",
            "Epoch 42/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.6363\n",
            "Epoch 43/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.6260\n",
            "Epoch 44/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.6175\n",
            "Epoch 45/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.6105\n",
            "Epoch 46/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.6041\n",
            "Epoch 47/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.5961\n",
            "Epoch 48/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.5877\n",
            "Epoch 49/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.5815\n",
            "Epoch 50/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.5731\n",
            "Epoch 51/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.5663\n",
            "Epoch 52/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.5595\n",
            "Epoch 53/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.5528\n",
            "Epoch 54/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.5460\n",
            "Epoch 55/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.5397\n",
            "Epoch 56/60\n",
            "138/138 [==============================] - 18s 133ms/step - loss: 1.5331\n",
            "Epoch 57/60\n",
            "138/138 [==============================] - 18s 132ms/step - loss: 1.5271\n",
            "Epoch 58/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.5203\n",
            "Epoch 59/60\n",
            "138/138 [==============================] - 18s 131ms/step - loss: 1.5141\n",
            "Epoch 60/60\n",
            "138/138 [==============================] - 18s 130ms/step - loss: 1.5084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f484f8ce5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM1w-HCoub1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae528ead-4411-4aaf-c3a2-6aae3d0482fd"
      },
      "source": [
        "generate_texts(temperatures)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###########\n",
            "Temperature: 0.1\n",
            "--- Generating with seed: \" praises bestowed on\n",
            "her i received as made to a possession of my own.  we calle\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            " praises bestowed on\n",
            "her i received as made to a possession of my own.  we called with the\n",
            "happiness of the latter and wele and the sun be any one consented the\n",
            "same scare of the surrow reternity of the same science and among the\n",
            "treatory was to come conversent the expetion of the works of the\n",
            "eveness of the death of the morner which i had and a suge parion of\n",
            "dear creatored an\n",
            "###########\n",
            "Temperature: 0.30000000000000004\n",
            "--- Generating with seed: \"boy, you will never see your father again; you must come with me.'\n",
            "\n",
            "\"'hideous mo\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "boy, you will never see your father again; you must come with me.'\n",
            "\n",
            "\"'hideous mountains and presently to my exerte to me.  i had contented that\n",
            "i was unound bestowed to the nect as it sarries that she sould her have\n",
            "been the crimes of the events which he had agother to the destruction\n",
            "of the events and then the thoughts the stranges are of the\n",
            "court a reasper it with a little s\n",
            "###########\n",
            "Temperature: 0.5000000000000001\n",
            "--- Generating with seed: \"isery, and soon the bolt  will fall which must ravish from you your\n",
            "happiness fo\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "isery, and soon the bolt  will fall which must ravish from you your\n",
            "happiness for me.  i was dear just several mountains she was sudeand\n",
            "accupted in the morth of the warkness that i was atcourden to the hambe\n",
            "and delight and the were was the cottage of the day and creature hy\n",
            "person said thes, away to engly and the door contented that i should\n",
            "speed at affection by an instrampi\n",
            "###########\n",
            "Temperature: 0.7000000000000001\n",
            "--- Generating with seed: \" mention it if it\n",
            "agitates you; but your father and cousin would be very happy i\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            " mention it if it\n",
            "agitates you; but your father and cousin would be very happy in death\n",
            "with precupe his earry for it would not his coptaged wear and strod\n",
            "joy every again to my near beath, and we thought and to she the\n",
            "crutters of he sughount at myself success grou dreathed him or filstat\n",
            "and convertation of beloved at an ascelt i exhrusted even for\n",
            "the manner, forever in the \n",
            "###########\n",
            "Temperature: 0.9000000000000001\n",
            "--- Generating with seed: \"ities by which it might have been\n",
            "placed in my pocket.  but here also i am check\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "ities by which it might have been\n",
            "placed in my pocket.  but here also i am checked by thw stronger, on\n",
            "this discovery of his hat been persony, and that hearll societion i\n",
            "onne can componed the cottage of the nourde and continured, you did not\n",
            "reculise it recide that it was even sensited rehal best and m. eyes,\n",
            "and her to me; my imagination i was accounted miserable and saken an\n",
            "###########\n",
            "Temperature: 1.1000000000000003\n",
            "--- Generating with seed: \"yed on the heart of felix and rendered\n",
            "him, when i first saw him, the most miser\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "yed on the heart of felix and rendered\n",
            "him, when i first saw him, the most miseryed the valuse.  the would pain it\n",
            "in a dedandsting shath onses (northded\n",
            "warmer; to the sun ease ont the restanting whom her adryor came to\n",
            "believe the dassibuts on sose classaly the eased atterported in the\n",
            "everserain shils--they admannalked, in quitting is i enjoaned with\n",
            "mind pasise\n",
            "contiat not \n",
            "###########\n",
            "Temperature: 1.3000000000000003\n",
            "--- Generating with seed: \"his abhorred presence on me to remind me of my task or to\n",
            "contemplate its progre\"\n",
            "\n",
            "--- Fragment generated by the model\n",
            "\n",
            "his abhorred presence on me to remind me of my task or to\n",
            "contemplate its progress, the first said strange weve arainleallyoages,\n",
            "overcancellyess, werasprouge but as path lo sty flow.  yot agathis\n",
            "brhalt, arthout fasm higher's elizabeth, that attinep.  o scene evil-simble\n",
            "themywings, hew i saw drefred by it--ist my alforapafiek my coption, anghe\n",
            "they kindrend enceiving nor? my \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3lFZlAduld7",
        "colab_type": "text"
      },
      "source": [
        "These results are the best that I could achieve. For low temperatures we can see a decent grammatical structure of the sentences. Verbs are being used more frequently. The results for high temperatures are similar to those of the initial model - made up words with realistic punctuation. A possible explanation why this model seems to perform better is that this model is deeper (due to more LSTM layers) and hence it has learned more advanced underlying statistical structures. Probably it has learnt some grammar by only looking at sequences of characters - very impressive. This statement seems to be true because I could observe the same tendency for other texts, in particular for german ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNfqyqttsfv0",
        "colab_type": "text"
      },
      "source": [
        "# 2 Extra Credit: Literature and Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bscyv-wzsfv2",
        "colab_type": "text"
      },
      "source": [
        "I will relate to the most relevant fragment (in my opinion):\n",
        "\n",
        "\"\n",
        "It's really what people call intuition and make such a fuss about. \n",
        "Intuition is like reading a word without having to spell it out. \n",
        "A child can't do that, because it has had so little experience. \n",
        "But a grown-up person knows the word because he's seen it often before.\n",
        "\"\n",
        "\n",
        "This passage strongly reminds me of what we're trying to achieve when we learn a language model by means of a neural network. We're trying to force the network to learn some kind of intuition about the language it's trained on, just like a child absorbs natural language - by being constantly exposed to linguistical input. So in the sense of statistics humans and networks are similar - the more data they get, the better they learn the model. The only difference (rather enormous, unfortunately for Machine Learning) is that a human being is able to take these statistics a few steps further than the most sophisticated and computationally advanced network as of today. A human is able to interpret such a data and imply in a very natural and so far obscure way how to apply this knowledge to infinite number of different real and abstract life situations. The machines are getting better at this but still far behind us. So the phrase \"reading a word without having to spell it out\" means that a person doesn't need a numerical (or whatever else) encoding of a word to be able to perceive it. The machines in contrary are helpless without a decent encoding.\n",
        "\n",
        "Another parallel to our assignment from this passage is hidden in comparing a child to a neural network at its first epochs. Both don't know much about the language - both just gather some first observations - statistics and context. When a child grows up, its knowledge is more similar to the powerful neural network that contains a comprehensive language model under the hood. But obviously, as I already mentioned, the deep learning, as we are familiar with it today, is quite limited and it cannot top the limitation set by statistics, even when it's not possible for a human to explain it clearly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3m2FGYcsfv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}